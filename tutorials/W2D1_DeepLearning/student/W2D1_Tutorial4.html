
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding — Neuromatch Academy: Computational Neuroscience</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.jpg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../W2D1_Outro.html" rel="next" title="Outro"/>
<link href="W2D1_Tutorial3.html" rel="prev" title="Tutorial 3: Building and Evaluating Normative Encoding Models"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.jpg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/student/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/student/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/W0D3_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/W0D4_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D5_Statistics/chapter_title.html">
   Statistics (W0D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/W0D5_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial4.html">
     Tutorial 4: Model Dicussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelingPractice/chapter_title.html">
   Modeling Practice (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/student/W1D2_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/W1D2_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_ModelFitting/chapter_title.html">
   Model Fitting (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/chapter_title.html">
   Generalized Linear Models (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Deep Learning (W2D1)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="../W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D1_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D1_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D1_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial4.html">
     Tutorial 4: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial3.html">
     Bonus Tutorial:Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial3.html">
     Tutorial 3: 1D Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial4.html">
     Tutorial 4: 2D Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial4.html">
     Tutorial 4: From Reinforcement Learning to Planning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/abstract_guidance.html">
   How to write an abstract
  </a>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W2D1_DeepLearning/student/W2D1_Tutorial4.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D1_DeepLearning/student/W2D1_Tutorial4.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval-and-loading">
     Data retrieval and loading
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-decoding-investigating-model-and-evaluating-performance">
   Section 1: Decoding - Investigating model and evaluating performance
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-peering-inside-the-decoding-model">
     Section 1.1: Peering inside the decoding model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-visualizing-weights">
       Coding Exercise 1.1: Visualizing weights
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-1-1-interpreting-weights">
       Think! 1.1: Interpreting weights
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-generalization-performance-with-test-data">
     Section 1.2: Generalization performance with test data
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-decoding-evaluating-improving-models">
   Section 2: Decoding - Evaluating &amp; improving models
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-model-criticism">
     Section 2.1: Model criticism
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id4">
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-2-1-delving-into-error-problems">
       Think! 2.1: Delving into error problems
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-improving-the-loss-function">
     Section 2.2: Improving the loss function
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-a-new-loss-function">
       Coding Exercise 2.2: A new loss function
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-regularization">
     Section 2.3: Regularization
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-regularization">
       Video 4: Regularization
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-3-add-regularization-to-training">
       Coding Exercise 2.3: Add regularization to training
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-encoding-convolutional-networks-for-encoding">
   Section 3: Encoding - Convolutional Networks for Encoding
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-convolutional-encoding-model">
     Video 1: Convolutional Encoding Model
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-neural-tuning-curves">
     Section 3.1: Neural tuning curves
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-adding-a-fully-connected-layer-to-create-encoding-model">
     Section 3.2: Adding a fully-connected layer to create encoding model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-2-number-of-unis-and-weights">
       Think! 3.2: Number of unis and weights
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-2-add-linear-layer">
       Coding Exercise 3.2: Add linear layer
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W2D1_DeepLearning/student/W2D1_Tutorial4.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="bonus-tutorial-diving-deeper-into-decoding-encoding">
<h1>Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding<a class="headerlink" href="#bonus-tutorial-diving-deeper-into-decoding-encoding" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 2, Day 1: Deep Learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators</strong>: Jorge A. Menendez, Carsen Stringer</p>
<p><strong>Content reviewers</strong>: Roozbeh Farhoodi,  Madineh Sarvestani, Kshitij Dwivedi, Spiros Chavlis, Ella Batty, Michael Waskom</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we’ll will dive deeper into our decoding model from Tutorial 1 and we will fit a convolutional neural network directly to neural activities.</p>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="data-retrieval-and-loading">
<h2>Data retrieval and loading<a class="headerlink" href="#data-retrieval-and-loading" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Data retrieval and loading</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s2">"W3D4_stringer_oribinned1.npz"</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/683xc/download"</span>
<span class="n">expected_md5</span> <span class="o">=</span> <span class="s2">"436599dfd8ebe6019f066c38aed20580"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Data download appears corrupted !!!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
        <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure Settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting Functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Plotting Functions</span>

<span class="k">def</span> <span class="nf">plot_data_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="p">):</span>
  <span class="sd">"""Visualize data matrix of neural responses using a heatmap</span>

<span class="sd">  Args:</span>
<span class="sd">    X (torch.Tensor or np.ndarray): matrix of neural responses to visualize</span>
<span class="sd">        with a heatmap</span>
<span class="sd">    ax (matplotlib axes): where to plot</span>

<span class="sd">  """</span>

  <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">pink</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">vmax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">99</span><span class="p">))</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'normalized neural response'</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'auto'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>

<span class="k">def</span> <span class="nf">plot_decoded_results</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">predicted_test_labels</span><span class="p">):</span>
  <span class="sd">""" Plot decoding results in the form of network training loss and test predictions</span>

<span class="sd">  Args:</span>
<span class="sd">    train_loss (list): training error over iterations</span>
<span class="sd">    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>
<span class="sd">    predicted_test_labels (torch.Tensor): n_test x 1 tensor with predicted orientations of the</span>
<span class="sd">      stimuli from decoding neural network</span>

<span class="sd">  """</span>

  <span class="c1"># Plot results</span>
  <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

  <span class="c1"># Plot the training loss over iterations of GD</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
  <span class="c1"># Plot the testing loss over iterations of GD</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'test loss'</span><span class="p">])</span>

  <span class="c1"># Plot true stimulus orientation vs. predicted class</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli_test</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">predicted_test_labels</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">)</span>

  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'iterations of gradient descent'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'negative log likelihood'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'true stimulus orientation ($^o$)'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'decoded orientation bin'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span><span class="p">))</span>
  <span class="n">class_bins</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">i</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span><span class="si">:</span><span class="s1"> .0f</span><span class="si">}</span><span class="s1">$^o$ - </span><span class="si">{</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span><span class="si">:</span><span class="s1"> .0f</span><span class="si">}</span><span class="s1">$^o$'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">class_bins</span><span class="p">);</span>

  <span class="c1"># Draw bin edges as vertical lines</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">())</span>  <span class="c1"># fix y-axis limits</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">lower</span><span class="p">,</span> <span class="n">lower</span><span class="p">],</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">(),</span> <span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">upper</span><span class="p">,</span> <span class="n">upper</span><span class="p">],</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">(),</span> <span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">visualize_weights</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">W_out</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'sorted neurons'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$W_</span><span class="si">{in}</span><span class="s1">$'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">W_out</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'output'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$W_</span><span class="si">{out}</span><span class="s1">$'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">visualize_hidden_units</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'sorted neurons'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$W_</span><span class="si">{in}</span><span class="s1">$'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^\circ$)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$\mathbf</span><span class="si">{h}</span><span class="s1">$'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^\circ$)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden unit activity'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$\mathbf</span><span class="si">{h}</span><span class="s1">$ tuning curves'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">""" plot convolutional channel weights</span>
<span class="sd">  Args:</span>
<span class="sd">        weights: weights of convolutional filters (conv_channels x K x K)</span>
<span class="sd">        channels: which conv channels to plot</span>
<span class="sd">  """</span>
  <span class="n">wmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">channels</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">channel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">channel</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=-</span><span class="n">wmax</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">wmax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'channel </span><span class="si">%d</span><span class="s1">'</span><span class="o">%</span><span class="k">channel</span>)

  <span class="k">if</span> <span class="n">colorbar</span><span class="p">:</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_tuning</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">stimuli</span><span class="p">,</span> <span class="n">respi_train</span><span class="p">,</span> <span class="n">respi_test</span><span class="p">,</span> <span class="n">neuron_index</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="sd">"""Plot the tuning curve of a neuron"""</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli</span><span class="p">,</span> <span class="n">respi_train</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">)</span>  <span class="c1"># plot its responses as a function of stimulus orientation</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli</span><span class="p">,</span> <span class="n">respi_test</span><span class="p">,</span> <span class="s1">'m'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">)</span>  <span class="c1"># plot its responses as a function of stimulus orientation</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'neuron </span><span class="si">%i</span><span class="s1">'</span> <span class="o">%</span> <span class="n">neuron_index</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^o$)'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'neural response'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">plot_prediction</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
  <span class="sd">""" plot prediction of neural response + test neural response """</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'m'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="s1">'g'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus bin'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'response'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_training_curves</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Args:</span>
<span class="sd">    train_loss (list): training error over iterations</span>
<span class="sd">    test_loss (list): n_test x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>
<span class="sd">    predicted_test_labels (torch.Tensor): n_test x 1 tensor with predicted orientations of the</span>
<span class="sd">      stimuli from decoding neural network</span>

<span class="sd">  """</span>

  <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="c1"># Plot the training loss over iterations of GD</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
  <span class="c1"># Plot the testing loss over iterations of GD</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'test loss'</span><span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">"Gradient descent iteration"</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">"Mean squared error"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper Functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions</span>

<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">data_name</span><span class="o">=</span><span class="n">fname</span><span class="p">,</span> <span class="n">bin_width</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">"""Load mouse V1 data from Stringer et al. (2019)</span>

<span class="sd">  Data from study reported in this preprint:</span>
<span class="sd">  https://www.biorxiv.org/content/10.1101/679324v2.abstract</span>

<span class="sd">  These data comprise time-averaged responses of ~20,000 neurons</span>
<span class="sd">  to ~4,000 stimulus gratings of different orientations, recorded</span>
<span class="sd">  through Calcium imaging. The responses have been normalized by</span>
<span class="sd">  spontaneous levels of activity and then z-scored over stimuli, so</span>
<span class="sd">  expect negative numbers. They have also been binned and averaged</span>
<span class="sd">  to each degree of orientation.</span>

<span class="sd">  This function returns the relevant data (neural responses and</span>
<span class="sd">  stimulus orientations) in a torch.Tensor of data type torch.float32</span>
<span class="sd">  in order to match the default data type for nn.Parameters in</span>
<span class="sd">  Google Colab.</span>

<span class="sd">  This function will actually average responses to stimuli with orientations</span>
<span class="sd">  falling within bins specified by the bin_width argument. This helps</span>
<span class="sd">  produce individual neural "responses" with smoother and more</span>
<span class="sd">  interpretable tuning curves.</span>

<span class="sd">  Args:</span>
<span class="sd">    bin_width (float): size of stimulus bins over which to average neural</span>
<span class="sd">      responses</span>

<span class="sd">  Returns:</span>
<span class="sd">    resp (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,</span>
<span class="sd">        each row contains the responses of each neuron to a given stimulus.</span>
<span class="sd">        As mentioned above, neural "response" is actually an average over</span>
<span class="sd">        responses to stimuli with similar angles falling within specified bins.</span>
<span class="sd">    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation</span>
<span class="sd">        of each stimulus, in degrees. This is actually the mean orientation</span>
<span class="sd">        of all stimuli in each bin.</span>

<span class="sd">  """</span>
  <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">dobj</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">dobj</span><span class="p">)</span>
  <span class="n">resp</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'resp'</span><span class="p">]</span>
  <span class="n">stimuli</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'stimuli'</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">bin_width</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Bin neural responses and stimuli</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">stimuli</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span> <span class="o">+</span> <span class="n">bin_width</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">))</span>
    <span class="n">stimuli_binned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">stimuli</span><span class="p">[</span><span class="n">bins</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">bins</span><span class="p">)])</span>
    <span class="n">resp_binned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">resp</span><span class="p">[</span><span class="n">bins</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">bins</span><span class="p">)])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">resp_binned</span> <span class="o">=</span> <span class="n">resp</span>
    <span class="n">stimuli_binned</span> <span class="o">=</span> <span class="n">stimuli</span>

  <span class="c1"># Return as torch.Tensor</span>
  <span class="n">resp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">resp_binned</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">stimuli_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">stimuli_binned</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># add singleton dimension to make a column vector</span>

  <span class="k">return</span> <span class="n">resp_tensor</span><span class="p">,</span> <span class="n">stimuli_tensor</span>


<span class="k">def</span> <span class="nf">identityLine</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Plot the identity line y=x</span>
<span class="sd">  """</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">lims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()])</span>
  <span class="n">minval</span> <span class="o">=</span> <span class="n">lims</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
  <span class="n">maxval</span> <span class="o">=</span> <span class="n">lims</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
  <span class="n">equal_lims</span> <span class="o">=</span> <span class="p">[</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">]</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">equal_lims</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">equal_lims</span><span class="p">)</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">],</span> <span class="p">[</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">)</span>
  <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_zorder</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">n_stim</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">):</span>
  <span class="sd">""" Return n_stim randomly drawn stimuli/resp pairs</span>

<span class="sd">  Args:</span>
<span class="sd">    n_stim (scalar): number of stimuli to draw</span>
<span class="sd">    resp (torch.Tensor):</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor, torch.Tensor): n_stim x n_neurons tensor of neural responses and n_stim x 1 of orientations respectively</span>
<span class="sd">  """</span>
  <span class="n">n_stimuli</span> <span class="o">=</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">istim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_stimuli</span><span class="p">,</span> <span class="n">n_stim</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">istim</span><span class="p">]</span>  <span class="c1"># neural responses to this stimulus</span>
  <span class="n">ori</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">istim</span><span class="p">]</span>  <span class="c1"># true stimulus orientation</span>

  <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">ori</span>

<span class="k">def</span> <span class="nf">stimulus_class</span><span class="p">(</span><span class="n">ori</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
  <span class="sd">"""Get stimulus class from stimulus orientation</span>

<span class="sd">  Args:</span>
<span class="sd">    ori (torch.Tensor): orientations of stimuli to return classes for</span>
<span class="sd">    n_classes (int): total number of classes</span>

<span class="sd">  Returns:</span>
<span class="sd">    torch.Tensor: 1D tensor with the classes for each stimulus</span>

<span class="sd">  """</span>
  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">ori</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">bins</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># minus 1 to accomodate Python indexing</span>

<span class="k">def</span> <span class="nf">grating</span><span class="p">(</span><span class="n">angle</span><span class="p">,</span> <span class="n">sf</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">28</span><span class="p">,</span> <span class="n">res</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patch</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">"""Generate oriented grating stimulus</span>

<span class="sd">  Args:</span>
<span class="sd">    angle (float): orientation of grating (angle from vertical), in degrees</span>
<span class="sd">    sf (float): controls spatial frequency of the grating</span>
<span class="sd">    res (float): resolution of image. Smaller values will make the image</span>
<span class="sd">      smaller in terms of pixels. res=1.0 corresponds to 640 x 480 pixels.</span>
<span class="sd">    patch (boolean): set to True to make the grating a localized</span>
<span class="sd">      patch on the left side of the image. If False, then the</span>
<span class="sd">      grating occupies the full image.</span>

<span class="sd">  Returns:</span>
<span class="sd">    torch.Tensor: (res * 480) x (res * 640) pixel oriented grating image</span>

<span class="sd">  """</span>

  <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>  <span class="c1"># transform to radians</span>

  <span class="n">wpix</span><span class="p">,</span> <span class="n">hpix</span> <span class="o">=</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">480</span>  <span class="c1"># width and height of image in pixels for res=1.0</span>

  <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">sf</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wpix</span> <span class="o">*</span> <span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="n">res</span><span class="p">,</span> <span class="n">sf</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hpix</span> <span class="o">*</span> <span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="n">res</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">patch</span><span class="p">:</span>
    <span class="n">gratings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">xx</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span> <span class="o">+</span> <span class="mf">.1</span><span class="p">)</span> <span class="o">+</span> <span class="n">yy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span> <span class="o">+</span> <span class="mf">.1</span><span class="p">))</span>  <span class="c1"># phase shift to make it better fit within patch</span>
    <span class="n">gratings</span><span class="p">[</span><span class="n">gratings</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">gratings</span><span class="p">[</span><span class="n">gratings</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">xcent</span> <span class="o">=</span> <span class="n">gratings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">.75</span>
    <span class="n">ycent</span> <span class="o">=</span> <span class="n">gratings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">xxc</span><span class="p">,</span> <span class="n">yyc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">gratings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">gratings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">icirc</span> <span class="o">=</span> <span class="p">((</span><span class="n">xxc</span> <span class="o">-</span> <span class="n">xcent</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">yyc</span> <span class="o">-</span> <span class="n">ycent</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span> <span class="o">&lt;</span> <span class="n">wpix</span> <span class="o">/</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">res</span>
    <span class="n">gratings</span><span class="p">[</span><span class="o">~</span><span class="n">icirc</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="n">gratings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">xx</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span> <span class="o">+</span> <span class="n">yy</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">))</span>
    <span class="n">gratings</span><span class="p">[</span><span class="n">gratings</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">gratings</span><span class="p">[</span><span class="n">gratings</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">gratings</span> <span class="o">-=</span> <span class="mf">0.5</span>

  <span class="c1"># Return torch tensor</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gratings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">filters</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
  <span class="sd">""" make example filters, some center-surround and gabors</span>
<span class="sd">  Returns:</span>
<span class="sd">      filters: out_channels x K x K</span>
<span class="sd">  """</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">K</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s1">'ij'</span><span class="p">)</span>

  <span class="c1"># create center-surround filters</span>
  <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.1</span>
  <span class="n">gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
  <span class="n">wide_gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">sigma</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
  <span class="n">center_surround</span> <span class="o">=</span> <span class="n">gaussian</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">wide_gaussian</span>

  <span class="c1"># create gabor filters</span>
  <span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">-</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span>
  <span class="n">gabors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">thetas</span><span class="p">),</span> <span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">lam</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span>
  <span class="n">gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">sigma</span><span class="o">*</span><span class="mf">0.4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">theta</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">thetas</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xx</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">yy</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">gabors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">gaussian</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="o">/</span><span class="n">lam</span> <span class="o">+</span> <span class="n">phi</span><span class="p">)</span>

  <span class="n">filters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">center_surround</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:],</span>
                            <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">center_surround</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:],</span>
                            <span class="n">gabors</span><span class="p">),</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">filters</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="n">filters</span> <span class="o">-=</span> <span class="n">filters</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="c1"># convert to torch</span>
  <span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>
  <span class="c1"># add channel axis</span>
  <span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">filters</span>

<span class="k">def</span> <span class="nf">regularized_MSE_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""loss function for MSE</span>

<span class="sd">  Args:</span>
<span class="sd">    output (torch.Tensor): output of network</span>
<span class="sd">    target (torch.Tensor): neural response network is trying to predict</span>
<span class="sd">    weights (torch.Tensor): fully-connected layer weights of network (net.out_layer.weight)</span>
<span class="sd">    L2_penalty : scaling factor of sum of squared weights</span>
<span class="sd">    L1_penalty : scalaing factor for sum of absolute weights</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor) mean-squared error with L1 and L2 penalties added</span>

<span class="sd">  """</span>

  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="n">L2_penalty</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="n">L1_penalty</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">L2</span>

  <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-decoding-investigating-model-and-evaluating-performance">
<h1>Section 1: Decoding - Investigating model and evaluating performance<a class="headerlink" href="#section-1-decoding-investigating-model-and-evaluating-performance" title="Permalink to this headline">¶</a></h1>
<p>In this section, we will return to our decoding model from Tutorial 1 and further investigate its performance, and then improve it in the next section. Let’s first load the data again and train our model, as we did in Tutorial 1.</p>
<div class="section" id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Execute this cell to load and visualize data</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Execute this cell to load and visualize data</span>

<span class="c1"># Load data</span>
<span class="n">resp_all</span><span class="p">,</span> <span class="n">stimuli_all</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>  <span class="c1"># argument to this function specifies bin width</span>
<span class="n">n_stimuli</span><span class="p">,</span> <span class="n">n_neurons</span> <span class="o">=</span> <span class="n">resp_all</span><span class="o">.</span><span class="n">shape</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">n_neurons</span><span class="si">}</span><span class="s1"> neurons in response to </span><span class="si">{</span><span class="n">n_stimuli</span><span class="si">}</span><span class="s1"> stimuli'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Visualize data matrix</span>
<span class="n">plot_data_matrix</span><span class="p">(</span><span class="n">resp_all</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>  <span class="c1"># plot responses of first 100 neurons</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'neuron'</span><span class="p">)</span>

<span class="c1"># Plot tuning curves of three random neurons</span>
<span class="n">ineurons</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># pick three random neurons</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli_all</span><span class="p">,</span> <span class="n">resp_all</span><span class="p">[:,</span> <span class="n">ineurons</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^o$)'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'neural response'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>23589 neurons in response to 360 stimuli
</pre></div>
</div>
<img alt="../../../_images/W2D1_Tutorial4_17_1.png" src="../../../_images/W2D1_Tutorial4_17_1.png"/>
</div>
</div>
</div>
<div class="section" id="id2">
<h2><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Execute this cell to split into training and test sets</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to split into training and test sets</span>

<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Split data into training set and testing set</span>
<span class="n">n_train</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.6</span> <span class="o">*</span> <span class="n">n_stimuli</span><span class="p">)</span>  <span class="c1"># use 60% of all data for training set</span>
<span class="n">ishuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n_stimuli</span><span class="p">)</span>
<span class="n">itrain</span> <span class="o">=</span> <span class="n">ishuffle</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]</span>  <span class="c1"># indices of data samples to include in training set</span>
<span class="n">itest</span> <span class="o">=</span> <span class="n">ishuffle</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>  <span class="c1"># indices of data samples to include in testing set</span>
<span class="n">stimuli_test</span> <span class="o">=</span> <span class="n">stimuli_all</span><span class="p">[</span><span class="n">itest</span><span class="p">]</span>
<span class="n">resp_test</span> <span class="o">=</span> <span class="n">resp_all</span><span class="p">[</span><span class="n">itest</span><span class="p">]</span>
<span class="n">stimuli_train</span> <span class="o">=</span> <span class="n">stimuli_all</span><span class="p">[</span><span class="n">itrain</span><span class="p">]</span>
<span class="n">resp_train</span> <span class="o">=</span> <span class="n">resp_all</span><span class="p">[</span><span class="n">itrain</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Execute this cell to train the network</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to train the network</span>

<span class="k">class</span> <span class="nc">DeepNetReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">""" network with a single hidden layer h with a RELU """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># needed to invoke the properties of the parent class nn.Module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="c1"># neural activity --&gt; hidden units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># hidden units --&gt; output</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">r</span><span class="p">))</span> <span class="c1"># h is size (n_inputs, n_hidden)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># y is size (n_inputs, 1)</span>

    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
          <span class="n">n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
  <span class="sd">"""Run gradient descent to opimize parameters of a given network</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): PyTorch network whose parameters to optimize</span>
<span class="sd">    loss_fn: built-in PyTorch loss function to minimize</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data</span>
<span class="sd">    n_epochs (int, optional): number of epochs of gradient descent to run</span>
<span class="sd">    learning_rate (float, optional): learning rate to use for gradient descent</span>

<span class="sd">  Returns:</span>
<span class="sd">    (list): training loss over iterations</span>

<span class="sd">  """</span>

  <span class="c1"># Initialize PyTorch SGD optimizer</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># Placeholder to save the loss at each iteration</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Loop over epochs</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>

    <span class="c1"># compute network output from inputs in train_data</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>  <span class="c1"># compute network output from inputs in train_data</span>

    <span class="c1"># evaluate loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

    <span class="c1"># Clear previous gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Compute gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update weights</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store current value of loss</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># .item() needed to transform the tensor output of loss_fn to a scalar</span>


    <span class="c1"># Track progress</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_epochs</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span>


<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize network with 10 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Initialize built-in PyTorch MSE loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Run gradient descent on data</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>

<span class="c1"># Plot the training loss over iterations of GD</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'iterations of gradient descent'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'mean squared error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration 10/50 | loss: 12219.761
iteration 20/50 | loss: 1672.732
iteration 30/50 | loss: 548.096
iteration 40/50 | loss: 235.619
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration 50/50 | loss: 144.001
</pre></div>
</div>
<img alt="../../../_images/W2D1_Tutorial4_22_2.png" src="../../../_images/W2D1_Tutorial4_22_2.png"/>
</div>
</div>
</div>
<div class="section" id="section-1-1-peering-inside-the-decoding-model">
<h2>Section 1.1: Peering inside the decoding model<a class="headerlink" href="#section-1-1-peering-inside-the-decoding-model" title="Permalink to this headline">¶</a></h2>
<p>We have built a model to perform decoding that takes as input neural activity and outputs the estimated angle of the stimulus. We can imagine that an animal that needs to determine angles would have a brain area that acts like the hidden layer in our model. It transforms the neural activity from visual cortex and outputs a decision. Decisions about orientations of edges could include figuring out how to jump onto a branch, how to avoid obstacles, or determining the type of an object, e.g. food or predator.</p>
<p>What sort of connectivity would this brain area have with visual cortex? Determining this experimentally would be very difficult, perhaps we can look at the model we have and see if its structure constrains the type of connectivity we’d expect.</p>
<p>Below we will visualize the weights from the neurons in visual cortex to the hidden units <span class="math notranslate nohighlight">\(\mathbf{W}_{in}\)</span>, and the weights from the hidden units to the output orientation <span class="math notranslate nohighlight">\(\mathbf{W}_{out}\)</span>.</p>
<p><strong>PyTorch Note</strong>:</p>
<p>An important thing to note in the code below is the <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> method. The PyTorch <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class is special in that, behind the scenes, each of the variables inside it are linked to each other in a computational graph, for the purposes of automatic differentiation (the algorithm used in <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> to compute gradients). As a result, if you want to do anything that is not a <code class="docutils literal notranslate"><span class="pre">torch</span></code> operation to the parameters or outputs of an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class, you’ll need to first “detach” it from its computational graph. This is what the <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> method does. In this code below, we need to call it on the weights of the network. We also convert the variable from a pytorch tensor to a numpy array using the <code class="docutils literal notranslate"><span class="pre">.numpy()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W_in</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># we can run .detach() and .numpy() to get a numpy array</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'shape of W_in:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W_in</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">W_out</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># we can run .detach() and .numpy() to get a numpy array</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'shape of W_out:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">W_in</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">'auto'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'neurons'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$W_</span><span class="si">{in}</span><span class="s1">$'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">W_out</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bwr'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'output'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'hidden units'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'$W_</span><span class="si">{out}</span><span class="s1">$'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>shape of W_in:
(10, 23589)
shape of W_out:
(1, 10)
</pre></div>
</div>
<img alt="../../../_images/W2D1_Tutorial4_25_1.png" src="../../../_images/W2D1_Tutorial4_25_1.png"/>
</div>
</div>
<div class="section" id="coding-exercise-1-1-visualizing-weights">
<h3>Coding Exercise 1.1: Visualizing weights<a class="headerlink" href="#coding-exercise-1-1-visualizing-weights" title="Permalink to this headline">¶</a></h3>
<p>It’s difficult to see any structure in this weight matrix. How might we visualize it in a better way?</p>
<p>Perhaps we can sort the neurons by their preferred orientation. We will use the <code class="docutils literal notranslate"><span class="pre">resp_all</span></code> matrix which is 360 stimuli (360<span class="math notranslate nohighlight">\(^\circ\)</span> of angles) by number of neurons. How do we find the preferred orientation?</p>
<p>Let’s visualize one column of this <code class="docutils literal notranslate"><span class="pre">resp_all</span></code> matrix first as we did at the beginning of the notebook. Can you see how we might want to first process this tuning curve before choosing the preferred orientation?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="mi">235</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">resp_all</span><span class="p">[:,</span><span class="n">idx</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'neural response'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^\circ$)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">'neuron </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W2D1_Tutorial4_27_0.png" src="../../../_images/W2D1_Tutorial4_27_0.png"/>
</div>
</div>
<p>Looking at this tuning curve, there is a bit of noise across orientations, so let’s smooth with a gaussian filter and then find the position of the maximum for each neuron. After getting the maximum position aka the “preferred orientation” for each neuron, we will re-sort the <span class="math notranslate nohighlight">\(\mathbf{W}_{in}\)</span> matrix. The maximum position in a matrix can be computed using the <code class="docutils literal notranslate"><span class="pre">.argmax(axis=_)</span></code> function in python – make sure you specify the right axis though! Next, to get the indices of a matrix sorted we will need to use the <code class="docutils literal notranslate"><span class="pre">.argsort()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter1d</span>

<span class="c1"># first let's smooth the tuning curves resp_all to make sure we get</span>
<span class="c1"># an accurate peak that isn't just noise</span>
<span class="c1"># resp_all is size (n_stimuli, n_neurons)</span>
<span class="n">resp_smoothed</span> <span class="o">=</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">resp_all</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># resp_smoothed is size (n_stimuli, n_neurons)</span>

<span class="c1">############################################################################</span>
<span class="c1">## TO DO for students</span>
<span class="c1"># Fill out function and remove</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: find preferred orientation"</span><span class="p">)</span>
<span class="c1">############################################################################</span>

<span class="c1">## find position of max response for each neuron</span>
<span class="c1">## aka preferred orientation for each neuron</span>
<span class="n">preferred_orientation</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">## Resort W_in matrix by preferred orientation</span>
<span class="n">isort</span> <span class="o">=</span> <span class="n">preferred_orientation</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
<span class="n">W_in_sorted</span> <span class="o">=</span> <span class="n">W_in</span><span class="p">[:,</span><span class="n">isort</span><span class="p">]</span>

<span class="c1"># plot resorted W_in matrix</span>
<span class="n">visualize_weights</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">W_out</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_fc318196.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_fc318196_0.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_fc318196_0.png" style="width: 1359.0px; height: 540.0px;"/></a>
<p>We can plot the activity of hidden units across various stimuli to better understand the hidden units. Recall the formula for the hidden units</p>
<div class="amsmath math notranslate nohighlight" id="equation-e5124dd7-2d54-4e88-ae4c-5118d3fd509d">
<span class="eqno">(153)<a class="headerlink" href="#equation-e5124dd7-2d54-4e88-ae4c-5118d3fd509d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \mathbf{h}^{(n)} = \phi(\mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in})
\end{equation}\]</div>
<p>We can compute the activity <span class="math notranslate nohighlight">\(\mathbf{h}^{(n)}\)</span> directly using <span class="math notranslate nohighlight">\(\mathbf{W}^{in}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}^{in}\)</span> or we can modify our network above to return <code class="docutils literal notranslate"><span class="pre">h</span></code> in the <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> method. In this case, we’ll compute using the equation, but in practice the second method is recommended.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W_in</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># size (10, n_neurons)</span>
<span class="n">b_in</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># size (10, 1)</span>

<span class="c1"># Compute hidden unit activity h</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">W_in</span> <span class="o">@</span> <span class="n">resp_all</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b_in</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># we can run .detach() and .numpy() to get a numpy array</span>

<span class="c1"># Visualize</span>
<span class="n">visualize_hidden_units</span><span class="p">(</span><span class="n">W_in_sorted</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="think-1-1-interpreting-weights">
<h3>Think! 1.1: Interpreting weights<a class="headerlink" href="#think-1-1-interpreting-weights" title="Permalink to this headline">¶</a></h3>
<p>We have just visualized how the model transforms neural activity to hidden layer activity. How should we interpret these matrices? Here are some guiding questions to explore:</p>
<ul class="simple">
<li><p>Why are some of the <span class="math notranslate nohighlight">\(\mathbf{W}_{in}\)</span> weights close to zero for some of the hidden units? Do these correspond to close to zero weights in <span class="math notranslate nohighlight">\(\mathbf{W}_{out}\)</span>?</p></li>
<li><p>Note how each hidden unit seems to have strongest weights to two groups of neurons in <span class="math notranslate nohighlight">\(\mathbf{W}_{in}\)</span>, corresponding to two different sets of preferred orientations. Why do you think that is? What does might this tell us about the structure of the tuning curves of the neurons?</p></li>
<li><p>It appears that there is at least one hidden unit active at each orientation, which is necessary to decode across all orientations. What would happen if some orientations did not activate any hidden units?</p></li>
</ul>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_4a594500.py"><em>Click for solution</em></a></p>
</div>
</div>
<div class="section" id="section-1-2-generalization-performance-with-test-data">
<h2>Section 1.2: Generalization performance with test data<a class="headerlink" href="#section-1-2-generalization-performance-with-test-data" title="Permalink to this headline">¶</a></h2>
<p>Note that gradient descent is essentially an algorithm for fitting the network’s parameters to a given set of training data. Selecting this training data is thus crucial for ensuring that the optimized parameters <strong>generalize</strong> to unseen data they weren’t trained on. In our case, for example, we want to make sure that our trained network is good at decoding stimulus orientations from neural responses to any orientation, not just those in our data set.</p>
<p>To ensure this, we have split up the full data set into a <strong>training set</strong> and a <strong>testing set</strong>. In Coding Exercise 3.2, we trained a deep network by optimizing the parameters on a training set. We will now evaluate how good the optimized parameters are by using the trained network to decode stimulus orientations from neural responses in the testing set. Good decoding performance on this testing set should then be indicative of good decoding performance on the neurons’ responses to any other stimulus orientation. This procedure is commonly used in machine learning (not just in deep learning)and is typically referred to as <strong>cross-validation</strong>.</p>
<p>We will compute the MSE on the test data and plot the decoded stimulus orientations as a function of the true stimulus.</p>
<div class="section" id="id3">
<h3><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Execute this cell to evaluate and plot test error</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to evaluate and plot test error</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">resp_test</span><span class="p">)</span>  <span class="c1"># decode stimulus orientation for neural responses in testing set</span>
<span class="n">ori</span> <span class="o">=</span> <span class="n">stimuli_test</span>  <span class="c1"># true stimulus orientations</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">ori</span><span class="p">)</span>  <span class="c1"># MSE on testing set (Hint: use loss_fn initialized in previous exercise)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ori</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="s1">'.'</span><span class="p">)</span>  <span class="c1"># N.B. need to use .detach() to pass network output into plt.plot()</span>
<span class="n">identityLine</span><span class="p">()</span>  <span class="c1"># draw the identity line y=x; deviations from this indicate bad decoding!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'MSE on testing set: </span><span class="si">%.2f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># N.B. need to use .item() to turn test_loss into a scalar</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'true stimulus orientation ($^o$)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'decoded stimulus orientation ($^o$)'</span><span class="p">)</span>
<span class="n">axticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">axticks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">axticks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W2D1_Tutorial4_38_0.png" src="../../../_images/W2D1_Tutorial4_38_0.png"/>
</div>
</div>
<p>If interested, please see the next section to think more about model criticism, improve the loss function accordingly, and add regularization.</p>
</div>
</div>
</div>
<div class="section" id="section-2-decoding-evaluating-improving-models">
<h1>Section 2: Decoding - Evaluating &amp; improving models<a class="headerlink" href="#section-2-decoding-evaluating-improving-models" title="Permalink to this headline">¶</a></h1>
<hr class="docutils"/>
<div class="section" id="section-2-1-model-criticism">
<h2>Section 2.1: Model criticism<a class="headerlink" href="#section-2-1-model-criticism" title="Permalink to this headline">¶</a></h2>
<p>Let’s now take a step back and think about how our model is succeeding/failing and how to improve it.</p>
<div class="section" id="id4">
<h3><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Execute this cell to plot decoding error</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to plot decoding error</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">resp_test</span><span class="p">)</span>  <span class="c1"># decode stimulus orientation for neural responses in testing set</span>
<span class="n">ori</span> <span class="o">=</span> <span class="n">stimuli_test</span>  <span class="c1"># true stimulus orientations</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">out</span> <span class="o">-</span> <span class="n">ori</span>  <span class="c1"># decoding error</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ori</span><span class="p">,</span> <span class="n">error</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="s1">'.'</span><span class="p">)</span>   <span class="c1"># plot decoding error as a function of true orientation (make sure all arguments to plt.plot() have been detached from PyTorch network!)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'true stimulus orientation ($^o$)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'decoding error ($^o$)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">360</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W2D1_Tutorial4_44_0.png" src="../../../_images/W2D1_Tutorial4_44_0.png"/>
</div>
</div>
</div>
<div class="section" id="think-2-1-delving-into-error-problems">
<h3>Think! 2.1: Delving into error problems<a class="headerlink" href="#think-2-1-delving-into-error-problems" title="Permalink to this headline">¶</a></h3>
<p>In the cell below, we will plot the <em>decoding error</em> for each neural response in the testing set. The decoding error is defined as the decoded stimulus orientation minus true stimulus orientation</p>
<div class="amsmath math notranslate nohighlight" id="equation-00f440f4-e092-4b13-9f9a-a14f731df675">
<span class="eqno">(154)<a class="headerlink" href="#equation-00f440f4-e092-4b13-9f9a-a14f731df675" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  \text{decoding error} = y^{(n)} - \tilde{y}^{(n)}
\end{equation}\]</div>
<p>In particular, we plot decoding error as a function of the true stimulus orientation.</p>
<ul class="simple">
<li><p>Are some stimulus orientations harder to decode than others?</p></li>
<li><p>If so, in what sense? Are the decoded orientations for these stimuli more variable and/or are they biased?</p></li>
<li><p>Can you explain this variability/bias? What makes these stimulus orientations different from the others?</p></li>
<li><p>(Will be addressed in next exercise) Can you think of a way to modify the deep network in order to avoid this?</p></li>
</ul>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_10c7128b.py"><em>Click for solution</em></a></p>
</div>
</div>
<div class="section" id="section-2-2-improving-the-loss-function">
<h2>Section 2.2: Improving the loss function<a class="headerlink" href="#section-2-2-improving-the-loss-function" title="Permalink to this headline">¶</a></h2>
<p>As illustrated in the previous exercise, the squared error is not a good loss function for circular quantities like angles, since two angles that are very close (e.g. <span class="math notranslate nohighlight">\(1^o\)</span> and <span class="math notranslate nohighlight">\(359^o\)</span>) might actually have a very large squared error.</p>
<p>Here, we’ll avoid this problem by changing our loss function to treat our decoding problem as a <strong>classification problem</strong>. Rather than estimating the <em>exact</em> angle of the stimulus, we’ll now aim to construct a decoder that classifies the stimulus into one of <span class="math notranslate nohighlight">\(C\)</span> classes, corresponding to different bins of angles of width <span class="math notranslate nohighlight">\(b = \frac{360}{C}\)</span>. The true class <span class="math notranslate nohighlight">\(\tilde{y}^{(n)}\)</span> of stimulus <span class="math notranslate nohighlight">\(i\)</span> is now given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-e78ebfdb-3284-4605-9a58-bb944ccbeb87">
<span class="eqno">(155)<a class="headerlink" href="#equation-e78ebfdb-3284-4605-9a58-bb944ccbeb87" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  \tilde{y}^{(n)} =
  \begin{cases}
    1 &amp;\text{if angle of stimulus $n$ is in the range } [0, b] \\
    2 &amp;\text{if angle of stimulus $n$ is in the range } [b, 2b] \\
    3 &amp;\text{if angle of stimulus $n$ is in the range } [2b, 3b] \\
    \vdots \\
    C &amp;\text{if angle of stimulus $n$ is in the range } [(C-1)b, 360]
  \end{cases}
\end{equation}\]</div>
<p>We have a helper function <code class="docutils literal notranslate"><span class="pre">stimulus_class</span></code> that will extract <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> stimulus classes for us from the stimulus orientations.</p>
<p>To decode the stimulus class from neural responses, we’ll use a deep network that outputs a <span class="math notranslate nohighlight">\(C\)</span>-dimensional vector of probabilities <span class="math notranslate nohighlight">\(\mathbf{p} = \begin{bmatrix} p_1, p_2, \ldots, p_C \end{bmatrix}^T\)</span>, corresponding to the estimated probabilities of the stimulus belonging to each class <span class="math notranslate nohighlight">\(1, 2, \ldots, C\)</span>.</p>
<p>To ensure the network’s outputs are indeed probabilities (i.e. they are positive numbers between 0 and 1, and sum to 1), we’ll use a <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> to transform the real-valued outputs from the hidden layer into probabilities. Letting <span class="math notranslate nohighlight">\(\sigma(\cdot)\)</span> denote this softmax function, the equations describing our network are</p>
<div class="amsmath math notranslate nohighlight" id="equation-43314b1b-cf03-49f9-86c5-081a294e1910">
<span class="eqno">(156)<a class="headerlink" href="#equation-43314b1b-cf03-49f9-86c5-081a294e1910" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \mathbf{h}^{(n)} &amp;= \phi(\mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in}), &amp;&amp; [\mathbf{W}^{in}: M \times N], \\
    \mathbf{p}^{(n)} &amp;= \sigma(\mathbf{W}^{out} \mathbf{h}^{(n)} + \mathbf{b}^{out}),  &amp;&amp; [\mathbf{W}^{out}: C \times M],
\end{align}\]</div>
<p>The decoded stimulus class is then given by that assigned the highest probability by the network:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d008900-e3a2-42e4-8ae9-dec6084c5fa0">
<span class="eqno">(157)<a class="headerlink" href="#equation-4d008900-e3a2-42e4-8ae9-dec6084c5fa0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  y^{(n)} = \underset{i}{\arg\max} \,\, p_i
\end{equation}\]</div>
<p>The softmax function can be implemented in PyTorch simply using <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code>.</p>
<p>Often <em>log</em> probabilities are easier to work with than actual probabilities, because probabilities tend to be very small numbers that computers have trouble representing. We’ll therefore actually use the logarithm of the softmax as the output of our network,</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d2f7355-48e0-4a91-ac83-e765dcb330db">
<span class="eqno">(158)<a class="headerlink" href="#equation-8d2f7355-48e0-4a91-ac83-e765dcb330db" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \mathbf{l}^{(n)} = \log \left( \mathbf{p}^{(n)} \right)
\end{equation}\]</div>
<p>which can implemented in PyTorch together with the softmax via an <code class="docutils literal notranslate"><span class="pre">nn.LogSoftmax</span></code> layer. The nice thing about the logarithmic function is that it’s <em>monotonic</em>, so if one probability is larger/smaller than another, then its logarithm is also larger/smaller than the other’s. We therefore have that</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e6962cc-e537-441b-b8bd-f612d6fbe66b">
<span class="eqno">(159)<a class="headerlink" href="#equation-8e6962cc-e537-441b-b8bd-f612d6fbe66b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  y^{(n)} = \underset{i}{\arg\max} \,\, p_i^{(n)} = \underset{i}{\arg\max} \, \log p_i^{(n)} = \underset{i}{\arg\max} \,\, l_i^{(n)}
\end{equation}\]</div>
<p>See the next cell for code for constructing a deep network with one hidden layer that of ReLU’s that outputs a vector of log probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Deep network for classification</span>
<span class="k">class</span> <span class="nc">DeepNetSoftmax</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""Deep Network with one hidden layer, for classification</span>

<span class="sd">  Args:</span>
<span class="sd">    n_inputs (int): number of input units</span>
<span class="sd">    n_hidden (int): number of units in hidden layer</span>
<span class="sd">    n_classes (int): number of outputs, i.e. number of classes to output</span>
<span class="sd">      probabilities for</span>

<span class="sd">  Attributes:</span>
<span class="sd">    in_layer (nn.Linear): weights and biases of input layer</span>
<span class="sd">    out_layer (nn.Linear): weights and biases of output layer</span>

<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># needed to invoke the properties of the parent class nn.Module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>  <span class="c1"># neural activity --&gt; hidden units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>  <span class="c1"># hidden units --&gt; outputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logprob</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># probabilities across columns should sum to 1 (each output row corresponds to a different input)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="sd">"""Predict stimulus orientation bin from neural responses</span>

<span class="sd">    Args:</span>
<span class="sd">      r (torch.Tensor): n_stimuli x n_inputs tensor with neural responses to n_stimuli</span>

<span class="sd">    Returns:</span>
<span class="sd">      torch.Tensor: n_stimuli x n_classes tensor with predicted class probabilities</span>

<span class="sd">    """</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logprob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">logp</span>
</pre></div>
</div>
</div>
</div>
<p>What should our loss function now be? Ideally, we want the probabilities outputted by our network to be such that the probability of the true stimulus class is high. One way to formalize this is to say that we want to maximize the <em>log</em> probability of the true stimulus class <span class="math notranslate nohighlight">\(\tilde{y}^{(n)}\)</span> under the class probabilities predicted by the network,</p>
<div class="amsmath math notranslate nohighlight" id="equation-3780f3ad-b3fa-4acd-827f-552c9c62f621">
<span class="eqno">(160)<a class="headerlink" href="#equation-3780f3ad-b3fa-4acd-827f-552c9c62f621" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  \log \left( \text{predicted probability of stimulus } n \text{ being of class } \tilde{y}^{(n)} \right) = \log p^{(n)}_{\tilde{y}^{(n)}} = l^{(n)}_{\tilde{y}^{(n)}}
\end{equation}\]</div>
<p>To turn this into a loss function to be <em>minimized</em>, we can then simply multiply it by -1: maximizing the log probability is the same as minimizing the <em>negative</em> log probability. Summing over a batch of <span class="math notranslate nohighlight">\(P\)</span> inputs, our loss function is then given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-c56a8b74-c66f-4402-a88e-6f6f63d8c36b">
<span class="eqno">(161)<a class="headerlink" href="#equation-c56a8b74-c66f-4402-a88e-6f6f63d8c36b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  L = -\sum_{n=1}^P \log p^{(n)}_{\tilde{y}^{(n)}} = -\sum_{n=1}^P l^{(n)}_{\tilde{y}^{(n)}}
\end{equation}\]</div>
<p>In the deep learning community, this loss function is typically referred to as the <strong>cross-entropy</strong>, or <strong>negative log likelihood</strong>. The corresponding built-in loss function in PyTorch is <code class="docutils literal notranslate"><span class="pre">nn.NLLLoss()</span></code> (documentation <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html">here</a>).</p>
<div class="section" id="coding-exercise-2-2-a-new-loss-function">
<h3>Coding Exercise 2.2: A new loss function<a class="headerlink" href="#coding-exercise-2-2-a-new-loss-function" title="Permalink to this headline">¶</a></h3>
<p>In the next cell, we’ve provided most of the code to train and test a network to decode stimulus orientations via classification, by minimizing the negative log likelihood. Fill in the missing pieces.</p>
<p>Once you’ve done this, have a look at the plotted results. Does changing the loss function from mean squared error to a classification loss solve our problems? Note that errors may still occur – but are these errors as bad as the ones that our network above was making?</p>
<p>Run this cell to create train function that uses test_data and L1 and L2 terms for next exercise</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Run this cell to create train function that uses test_data and L1 and L2 terms for next exercise</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
          <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
          <span class="n">test_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">L2_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""Run gradient descent to opimize parameters of a given network</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): PyTorch network whose parameters to optimize</span>
<span class="sd">    loss_fn: built-in PyTorch loss function to minimize</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data</span>
<span class="sd">    n_iter (int, optional): number of iterations of gradient descent to run</span>
<span class="sd">    learning_rate (float, optional): learning rate to use for gradient descent</span>
<span class="sd">    test_data (torch.Tensor, optional): n_test x n_neurons tensor with neural</span>
<span class="sd">      responses to test on</span>
<span class="sd">    test_labels (torch.Tensor, optional): n_test x 1 tensor with orientations of</span>
<span class="sd">      the stimuli corresponding to each row of test_data</span>
<span class="sd">    L2_penalty (float, optional): l2 penalty regularizer coefficient</span>
<span class="sd">    L1_penalty (float, optional): l1 penalty regularizer coefficient</span>

<span class="sd">  Returns:</span>
<span class="sd">    (list): training loss over iterations</span>

<span class="sd">  """</span>

  <span class="c1"># Initialize PyTorch SGD optimizer</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># Placeholder to save the loss at each iteration</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Loop over epochs</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>

    <span class="c1"># compute network output from inputs in train_data</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>  <span class="c1"># compute network output from inputs in train_data</span>

    <span class="c1"># evaluate loss function</span>
    <span class="k">if</span> <span class="n">L2_penalty</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">L1_penalty</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="c1"># normal loss function</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># custom loss function from bonus exercise 3.3</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                     <span class="n">L2_penalty</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="p">)</span>

    <span class="c1"># Clear previous gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Compute gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update weights</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store current value of loss</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># .item() needed to transform the tensor output of loss_fn to a scalar</span>

    <span class="c1"># Get loss for test_data, if given (we will use this in the bonus exercise 3.2 and 3.3)</span>
    <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">out_test</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
      <span class="c1"># evaluate loss function</span>
      <span class="k">if</span> <span class="n">L2_penalty</span><span class="o">==</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">L1_penalty</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="c1"># normal loss function</span>
        <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out_test</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># (BONUS code) custom loss function from Bonus exercise 3.3</span>
        <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out_test</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                            <span class="n">L2_penalty</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="p">)</span>
      <span class="n">test_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_test</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># .item() needed to transform the tensor output of loss_fn to a scalar</span>

    <span class="c1"># Track progress</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_iter</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> | test_loss: </span><span class="si">{</span><span class="n">loss_test</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">train_loss</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_orientation</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span>
                       <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span>
                       <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">""" Initialize, train, and test deep network to decode binned orientation from neural responses</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): deep network to run</span>
<span class="sd">    n_classes (scalar): number of classes in which to bin orientation</span>
<span class="sd">    loss_fn (function): loss function to run</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>
<span class="sd">    test_data (torch.Tensor): n_test x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>
<span class="sd">    n_iter (int, optional): number of iterations to run optimization</span>
<span class="sd">    L2_penalty (float, optional): l2 penalty regularizer coefficient</span>
<span class="sd">    L1_penalty (float, optional): l1 penalty regularizer coefficient</span>

<span class="sd">  Returns:</span>
<span class="sd">    (list, torch.Tensor): training loss over iterations, n_test x 1 tensor with predicted orientations of the</span>
<span class="sd">      stimuli from decoding neural network</span>
<span class="sd">  """</span>

  <span class="c1"># Bin stimulus orientations in training set</span>
  <span class="n">train_binned_labels</span> <span class="o">=</span> <span class="n">stimulus_class</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
  <span class="n">test_binned_labels</span> <span class="o">=</span> <span class="n">stimulus_class</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>


  <span class="c1"># Run GD on training set data, using learning rate of 0.1</span>
  <span class="c1"># (add optional arguments test_data and test_binned_labels!)</span>
  <span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_binned_labels</span><span class="p">,</span>
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
                                <span class="n">test_labels</span><span class="o">=</span><span class="n">test_binned_labels</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">,</span>
                                <span class="n">L2_penalty</span><span class="o">=</span><span class="n">L2_penalty</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="n">L1_penalty</span><span class="p">)</span>

  <span class="c1"># Decode neural responses in testing set data</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
  <span class="n">out_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># predicted classes</span>

  <span class="n">frac_correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_labels</span><span class="o">==</span><span class="n">test_binned_labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_binned_labels</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'&gt;&gt;&gt; fraction correct = </span><span class="si">{</span><span class="n">frac_correct</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">out_labels</span>

<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1">############################################################################</span>
<span class="c1">## TO DO for students</span>
<span class="c1"># Fill out function and remove</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: make network and loss"</span><span class="p">)</span>
<span class="c1">############################################################################</span>

<span class="c1"># Initialize network</span>
<span class="n">net</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># use M=20 hidden units</span>

<span class="c1"># Initialize built-in PyTorch negative log likelihood loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># Train network and run it on test images</span>
<span class="c1"># this function uses the train function you wrote before</span>
<span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">predicted_test_labels</span> <span class="o">=</span> <span class="n">decode_orientation</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span>
                                                                  <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">,</span> <span class="n">resp_test</span><span class="p">,</span> <span class="n">stimuli_test</span><span class="p">)</span>

<span class="c1"># Plot results</span>
<span class="n">plot_decoded_results</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">stimuli_test</span><span class="p">,</span> <span class="n">predicted_test_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_b90e0d13.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b90e0d13_5.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b90e0d13_5.png" style="width: 2268.0px; height: 827.0px;"/></a>
<p>How do the weights <span class="math notranslate nohighlight">\(W_{in}\)</span> from the neurons to the hidden layer look now?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W_in</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># we can run detach and numpy to get a numpy array</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'shape of W_in:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W_in</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">W_out</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># plot resorted W_in matrix</span>
<span class="n">visualize_weights</span><span class="p">(</span><span class="n">W_in</span><span class="p">[:,</span><span class="n">isort</span><span class="p">],</span> <span class="n">W_out</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<div class="section" id="section-2-3-regularization">
<h2>Section 2.3: Regularization<a class="headerlink" href="#section-2-3-regularization" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-4-regularization">
<h3>Video 4: Regularization<a class="headerlink" href="#video-4-regularization" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Video available at https://youtube.com/watch?v=Qnn5OPHKo5w
</pre></div>
</div>
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="410" src="https://www.youtube.com/embed/Qnn5OPHKo5w?fs=1" width="730"></iframe>
</div></div>
</div>
<p>As discussed in the lecture, it is often important to incorporate regularization terms into the loss function to avoid overfitting. In particular, in this case, we will use these terms to enforce sparsity in the linear layer from neurons to hidden units.</p>
<p>Here we’ll consider the classic L2 regularization penalty <span class="math notranslate nohighlight">\(\mathcal{R}_{L2}\)</span>, which is the sum of squares of each weight in the network <span class="math notranslate nohighlight">\(\sum_{ij} {\mathbf{W}^{out}_{ij}}^2\)</span> times a constant that we call <code class="docutils literal notranslate"><span class="pre">L2_penalty</span></code>.</p>
<p>We will also add an L1 regularization penalty <span class="math notranslate nohighlight">\(\mathcal{R}_{L1}\)</span> to enforce sparsity of the weights, which is the sum of the absolute values of the weights <span class="math notranslate nohighlight">\(\sum_{ij} |{\mathbf{W}^{out}_{ij}}|\)</span> times a constant that we call <code class="docutils literal notranslate"><span class="pre">L1_penalty</span></code>.</p>
<p>We will add both of these to the loss function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8a176851-ac82-4684-8871-5b0ab9a89ed1">
<span class="eqno">(162)<a class="headerlink" href="#equation-8a176851-ac82-4684-8871-5b0ab9a89ed1" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    L = (y - \tilde{y})^2 + \mathcal{R}_{L2} + \mathcal{R}_{L1}
\end{equation}\]</div>
<p>The parameters <code class="docutils literal notranslate"><span class="pre">L2_penalty</span></code> and <code class="docutils literal notranslate"><span class="pre">L1_penalty</span></code> are inputs to the train function.</p>
</div>
<div class="section" id="coding-exercise-2-3-add-regularization-to-training">
<h3>Coding Exercise 2.3: Add regularization to training<a class="headerlink" href="#coding-exercise-2-3-add-regularization-to-training" title="Permalink to this headline">¶</a></h3>
<p>We will create a new loss function that adds L1 and L2 regularization.
In particular, you will:</p>
<ul class="simple">
<li><p>add L2 loss penalty to the weights</p></li>
<li><p>add L1 loss penalty to the weights</p></li>
</ul>
<p>We will then train the network using this loss function. Full training will take a few minutes: if you want to train for just a few steps to speed up the code while iterating on your code, you can decrease the n_iter input from 500.</p>
<p>Hint: since we are using <code class="docutils literal notranslate"><span class="pre">torch</span></code> instead of <code class="docutils literal notranslate"><span class="pre">np</span></code>, we will use <code class="docutils literal notranslate"><span class="pre">torch.abs</span></code> instead of <code class="docutils literal notranslate"><span class="pre">np.absolute</span></code>. You can use <code class="docutils literal notranslate"><span class="pre">torch.sum</span></code> or <code class="docutils literal notranslate"><span class="pre">.sum()</span></code> to sum over a tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">regularized_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">"""loss function with L2 and L1 regularization</span>

<span class="sd">  Args:</span>
<span class="sd">    output (torch.Tensor): output of network</span>
<span class="sd">    target (torch.Tensor): neural response network is trying to predict</span>
<span class="sd">    weights (torch.Tensor): linear layer weights from neurons to hidden units (net.in_layer.weight)</span>
<span class="sd">    L2_penalty : scaling factor of sum of squared weights</span>
<span class="sd">    L1_penalty : scalaing factor for sum of absolute weights</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor) mean-squared error with L1 and L2 penalties added</span>

<span class="sd">  """</span>

  <span class="c1">##############################################################################</span>
  <span class="c1"># TO DO: add L1 and L2 regularization to the loss function</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete regularized_loss"</span><span class="p">)</span>
  <span class="c1">##############################################################################</span>

  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

  <span class="n">L2</span> <span class="o">=</span> <span class="n">L2_penalty</span> <span class="o">*</span> <span class="o">...</span>
  <span class="n">L1</span> <span class="o">=</span> <span class="n">L1_penalty</span> <span class="o">*</span> <span class="o">...</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">L2</span>

  <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Initialize network</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetSoftmax</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>  <span class="c1"># use M=20 hidden units</span>

<span class="c1"># Here you can play with L2_penalty &gt; 0, L1_penalty &gt; 0</span>
<span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">predicted_test_labels</span> <span class="o">=</span> <span class="n">decode_orientation</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span>
                                                                  <span class="n">regularized_loss</span><span class="p">,</span>
                                                                  <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">,</span>
                                                                  <span class="n">resp_test</span><span class="p">,</span> <span class="n">stimuli_test</span><span class="p">,</span>
                                                                  <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                                                  <span class="n">L2_penalty</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
                                                                  <span class="n">L1_penalty</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

<span class="c1"># Plot results</span>
<span class="n">plot_decoded_results</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">stimuli_test</span><span class="p">,</span> <span class="n">predicted_test_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_b91e57fd.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b91e57fd_5.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b91e57fd_5.png" style="width: 2268.0px; height: 827.0px;"/></a>
<p>It seems we were overfitting a little because we increased the accuracy a small amount by adding an L1 and L2 regularization penalty. What errors are still being made by the model?</p>
<p>Let’s see how the weights look after adding <code class="docutils literal notranslate"><span class="pre">L1_penalty</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W_in</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">in_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># we can run detach and numpy to get a numpy array</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'shape of W_in:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W_in</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">W_out</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">visualize_weights</span><span class="p">(</span><span class="n">W_in</span><span class="p">[:,</span><span class="n">isort</span><span class="p">],</span> <span class="n">W_out</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>The weights appear to be sparser than before.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-encoding-convolutional-networks-for-encoding">
<h1>Section 3: Encoding - Convolutional Networks for Encoding<a class="headerlink" href="#section-3-encoding-convolutional-networks-for-encoding" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-convolutional-encoding-model">
<h2>Video 1: Convolutional Encoding Model<a class="headerlink" href="#video-1-convolutional-encoding-model" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "5d93fbc04ceb4455a1d28d6b4a12eff0"}
</script></div>
</div>
<p>In neuroscience, we often want to understand how the brain represents external stimuli. One approach to discovering these representations is to build an encoding model that takes as input the external stimuli (in this case grating stimuli) and outputs the neural responses.</p>
<p>Because visual cortex is often thought to be a convolutional network where the same filters are combined across the visual field, we will use a model with a convolutional layer. We learned how to build a convolutional layer in the previous section. We will add to this convolutional layer a fully connected layer from the output of the convolutions to the neurons. We will then visualize the weights of this fully connected layer.</p>
<p>Execute this cell to load data</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to load data</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s2">"W3D4_stringer_oribinned6_split.npz"</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/p3aeb/download"</span>
<span class="n">expected_md5</span> <span class="o">=</span> <span class="s2">"b3f7245c6221234a676b71a1f43c3bb5"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Data download appears corrupted !!!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
        <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_data_split</span><span class="p">(</span><span class="n">data_name</span><span class="o">=</span><span class="n">fname</span><span class="p">):</span>
  <span class="sd">"""Load mouse V1 data from Stringer et al. (2019)</span>

<span class="sd">  Data from study reported in this preprint:</span>
<span class="sd">  https://www.biorxiv.org/content/10.1101/679324v2.abstract</span>

<span class="sd">  These data comprise time-averaged responses of ~20,000 neurons</span>
<span class="sd">  to ~4,000 stimulus gratings of different orientations, recorded</span>
<span class="sd">  through Calcium imaginge. The responses have been normalized by</span>
<span class="sd">  spontaneous levels of activity and then z-scored over stimuli, so</span>
<span class="sd">  expect negative numbers. The repsonses were split into train and</span>
<span class="sd">  test and then each set were averaged in bins of 6 degrees.</span>

<span class="sd">  This function returns the relevant data (neural responses and</span>
<span class="sd">  stimulus orientations) in a torch.Tensor of data type torch.float32</span>
<span class="sd">  in order to match the default data type for nn.Parameters in</span>
<span class="sd">  Google Colab.</span>

<span class="sd">  It will hold out some of the trials when averaging to allow us to have test</span>
<span class="sd">  tuning curves.</span>

<span class="sd">  Args:</span>
<span class="sd">    data_name (str): filename to load</span>

<span class="sd">  Returns:</span>
<span class="sd">    resp_train (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,</span>
<span class="sd">        each row contains the responses of each neuron to a given stimulus.</span>
<span class="sd">        As mentioned above, neural "response" is actually an average over</span>
<span class="sd">        responses to stimuli with similar angles falling within specified bins.</span>
<span class="sd">    resp_test (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,</span>
<span class="sd">        each row contains the responses of each neuron to a given stimulus.</span>
<span class="sd">        As mentioned above, neural "response" is actually an average over</span>
<span class="sd">        responses to stimuli with similar angles falling within specified bins</span>
<span class="sd">    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation</span>
<span class="sd">        of each stimulus, in degrees. This is actually the mean orientation</span>
<span class="sd">        of all stimuli in each bin.</span>

<span class="sd">  """</span>
  <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">dobj</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">dobj</span><span class="p">)</span>
  <span class="n">resp_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'resp_train'</span><span class="p">]</span>
  <span class="n">resp_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'resp_test'</span><span class="p">]</span>
  <span class="n">stimuli</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'stimuli'</span><span class="p">]</span>

  <span class="c1"># Return as torch.Tensor</span>
  <span class="n">resp_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">resp_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">resp_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">resp_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">stimuli_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">stimuli</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">resp_train_tensor</span><span class="p">,</span> <span class="n">resp_test_tensor</span><span class="p">,</span> <span class="n">stimuli_tensor</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-3-1-neural-tuning-curves">
<h2>Section 3.1: Neural tuning curves<a class="headerlink" href="#section-3-1-neural-tuning-curves" title="Permalink to this headline">¶</a></h2>
<p>In the next cell, we plot the turning curves of a random subset of neurons. We have binned the stimuli orientations more than in Tutorial 1. We create the gratings images as above for the 60 orientations below, and save them to a variable <code class="docutils literal notranslate"><span class="pre">grating_stimuli</span></code>.</p>
<p>Rerun the cell to look at different example neurons and observe the diversity of tuning curves in the population. How can we fit these neural responses with an encoding model?</p>
<p>Execute this cell to load data, create stimuli, and plot neural tuning curves</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to load data, create stimuli, and plot neural tuning curves</span>

<span class="c1">### Load data and bin at 8 degrees</span>
<span class="c1"># responses are split into test and train</span>
<span class="n">resp_train</span><span class="p">,</span> <span class="n">resp_test</span><span class="p">,</span> <span class="n">stimuli</span> <span class="o">=</span> <span class="n">load_data_split</span><span class="p">()</span>
<span class="n">n_stimuli</span><span class="p">,</span> <span class="n">n_neurons</span> <span class="o">=</span> <span class="n">resp_train</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'resp_train contains averaged responses of </span><span class="si">%i</span><span class="s1"> neurons to </span><span class="si">%i</span><span class="s1"> binned stimuli'</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_stimuli</span><span class="p">))</span>
<span class="c1">#print(resp_train.shape)</span>

<span class="c1"># also make stimuli into images</span>
<span class="n">orientations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">61</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">180</span>
<span class="n">grating_stimuli</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">60</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ori</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orientations</span><span class="p">):</span>
  <span class="n">grating_stimuli</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">grating</span><span class="p">(</span><span class="n">ori</span><span class="p">,</span> <span class="n">res</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)</span><span class="c1">#[18:30, 24:40]</span>

<span class="n">grating_stimuli</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">grating_stimuli</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'grating_stimuli contains 60 stimuli of size 12 x 16'</span><span class="p">)</span>

<span class="c1"># Visualize tuning curves</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
  <span class="n">neuron_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">)</span>  <span class="c1"># pick random neuron</span>
  <span class="n">plot_tuning</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">stimuli</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">[:,</span> <span class="n">neuron_index</span><span class="p">],</span> <span class="n">resp_test</span><span class="p">[:,</span> <span class="n">neuron_index</span><span class="p">],</span> <span class="n">neuron_index</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="s1">'test'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'m'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>resp_train contains averaged responses of 23589 neurons to 60 binned stimuli
grating_stimuli contains 60 stimuli of size 12 x 16
</pre></div>
</div>
<img alt="../../../_images/W2D1_Tutorial4_76_1.png" src="../../../_images/W2D1_Tutorial4_76_1.png"/>
</div>
</div>
</div>
<div class="section" id="section-3-2-adding-a-fully-connected-layer-to-create-encoding-model">
<h2>Section 3.2: Adding a fully-connected layer to create encoding model<a class="headerlink" href="#section-3-2-adding-a-fully-connected-layer-to-create-encoding-model" title="Permalink to this headline">¶</a></h2>
<p>We will build a torch model like above with a convolutional layer. Additionally, we will add a fully connected linear layer from the convolutional units to the neurons. We will use 6 convolutional channels (<span class="math notranslate nohighlight">\(C^{out}\)</span>) and a kernel size (<span class="math notranslate nohighlight">\(K\)</span>) of 7 with a stride of 1 and padding of <span class="math notranslate nohighlight">\(K/2\)</span> (same as above). The stimulus is size <code class="docutils literal notranslate"><span class="pre">(12,</span> <span class="pre">16)</span></code>. Then the convolutional unit activations will go through a linear layer to be transformed into neural responses.</p>
<div class="section" id="think-3-2-number-of-unis-and-weights">
<h3>Think! 3.2: Number of unis and weights<a class="headerlink" href="#think-3-2-number-of-unis-and-weights" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>How many units will the convolutional layer have?</p></li>
<li><p>How many weights will the fully connected linear layer from convolutional units to neurons have?</p></li>
</ul>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_c35524a1.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="coding-exercise-3-2-add-linear-layer">
<h3>Coding Exercise 3.2: Add linear layer<a class="headerlink" href="#coding-exercise-3-2-add-linear-layer" title="Permalink to this headline">¶</a></h3>
<p>Remember in Tutorial 1 we used linear layers. Use your knowledge from Tutorial 1 to add a linear layer to the model we created above.</p>
<p>Execute to get <code class="docutils literal notranslate"><span class="pre">train</span></code> function for our neural encoding model</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to get `train` function for our neural encoding model</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">custom_loss</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
          <span class="n">test_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">learning_rate</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
  <span class="sd">"""Run gradient descent for network without batches</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): deep network whose parameters to optimize with SGD</span>
<span class="sd">    custom_loss: loss function for network</span>
<span class="sd">    train_data: training data (n_train x input features)</span>
<span class="sd">    train_labels: training labels (n_train x output features)</span>
<span class="sd">    test_data: test data (n_train x input features)</span>
<span class="sd">    test_labels: test labels (n_train x output features)</span>
<span class="sd">    learning_rate (float): learning rate for gradient descent</span>
<span class="sd">    n_epochs (int): number of epochs to run gradient descent</span>
<span class="sd">    L2_penalty (float): magnitude of L2 penalty</span>
<span class="sd">    L1_penalty (float): magnitude of L1 penalty</span>

<span class="sd">  Returns:</span>
<span class="sd">    train_loss: training loss across iterations</span>
<span class="sd">    test_loss: testing loss across iterations</span>

<span class="sd">  """</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1"># Initialize PyTorch SGD optimizer</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>  <span class="c1"># Placeholder for train loss</span>
  <span class="n">test_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>  <span class="c1"># Placeholder for test loss</span>

  <span class="c1"># Loop over epochs</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="mi">10</span><span class="p">)[</span><span class="n">n_iter</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span> <span class="c1"># Forward pass: compute predicted y by passing train_data to the model.</span>

    <span class="k">if</span> <span class="n">L2_penalty</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">L1_penalty</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

    <span class="c1">### Update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero out gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Backward pass: compute gradient of the loss with respect to model parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># step parameters in gradient direction</span>
    <span class="n">train_loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># .item() transforms the tensor to a scalar and does .detach() for us</span>

    <span class="c1"># Track progress</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_iter</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">test_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">L2_penalty</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">L1_penalty</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">L2_penalty</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s1"> | train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> | test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s1"> | train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ConvFC</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""Deep network with one convolutional layer + one fully connected layer</span>

<span class="sd">  Attributes:</span>
<span class="sd">    conv (nn.Conv1d): convolutional layer</span>
<span class="sd">    dims (tuple): shape of convolutional layer output</span>
<span class="sd">    out_layer (nn.Linear): linear layer</span>

<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">12</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span>
               <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" initialize layer</span>
<span class="sd">    Args:</span>
<span class="sd">        c_in: number of input stimulus channels</span>
<span class="sd">        c_out: number of convolutional channels</span>
<span class="sd">        K: size of each convolutional filter</span>
<span class="sd">        h: number of stimulus bins, n_bins</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">K</span><span class="p">,</span>
                          <span class="n">padding</span><span class="o">=</span><span class="n">K</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_out</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># dimensions of conv layer output</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">)</span> <span class="c1"># number of hidden units</span>

    <span class="c1">################################################################################</span>
    <span class="c1">## TO DO for students: add fully connected layer to network (self.out_layer)</span>
    <span class="c1"># Fill out function and remove</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: add fully connected layer to initialize network"</span><span class="p">)</span>
    <span class="c1">################################################################################</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

    <span class="c1"># initialize weights</span>
    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">c_out</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># initialize weights to be small</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="sd">""" Predict neural responses to stimuli s</span>

<span class="sd">    Args:</span>
<span class="sd">        s (torch.Tensor): n_stimuli x c_in x h x w tensor with stimuli</span>

<span class="sd">    Returns:</span>
<span class="sd">        y (torch.Tensor): n_stimuli x n_neurons tensor with predicted neural responses</span>

<span class="sd">    """</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>  <span class="c1"># output of convolutional layer</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">))</span>  <span class="c1"># flatten each convolutional layer output into a vector</span>

    <span class="c1">################################################################################</span>
    <span class="c1">## TO DO for students: add fully connected layer to forward pass of network (self.out_layer)</span>
    <span class="c1"># Fill out function and remove</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: add fully connected layer to network"</span><span class="p">)</span>
    <span class="c1">################################################################################</span>
    <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>

    <span class="k">return</span> <span class="n">y</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cpu'</span><span class="p">)</span>

<span class="c1"># (Optional) To speed up processing, go to "Runtime" menu and "Change runtime"</span>
<span class="c1"># and select GPU processing, then uncomment line below, otherwise runtime will</span>
<span class="c1"># be ~ 2 minutes</span>
<span class="c1"># device = torch.device('cuda')</span>

<span class="c1"># Initialize network</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="n">resp_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## Initialize with filters from Tutorial 2</span>
<span class="n">example_filters</span> <span class="o">=</span> <span class="n">filters</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ConvFC</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">filters</span> <span class="o">=</span> <span class="n">example_filters</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Run GD on training set data</span>
<span class="c1"># ** this time we are also providing the test data to estimate the test loss</span>
<span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">regularized_MSE_loss</span><span class="p">,</span>
                              <span class="n">train_data</span><span class="o">=</span><span class="n">grating_stimuli</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">train_labels</span><span class="o">=</span><span class="n">resp_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                              <span class="n">test_data</span><span class="o">=</span><span class="n">grating_stimuli</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">test_labels</span><span class="o">=</span><span class="n">resp_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                              <span class="n">n_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                              <span class="n">L2_penalty</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">L1_penalty</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plot_training_curves</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_DeepLearning/solutions/W2D1_Tutorial4_Solution_b1d22ad3.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b1d22ad3_11.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_DeepLearning/static/W2D1_Tutorial4_Solution_b1d22ad3_11.png" style="width: 1116.0px; height: 828.0px;"/></a>
<p>How well can we fit single neuron tuning curves with this model? What aspects of the tuning curves are we capturing?</p>
<p>Execute this cell to examine predictions for random subsets of neurons</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to examine predictions for random subsets of neurons</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grating_stimuli</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="c1"># Visualize tuning curves &amp; plot neural predictions</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
  <span class="n">ineur</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">)</span>
  <span class="n">plot_prediction</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">[:,</span><span class="n">ineur</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                  <span class="n">resp_train</span><span class="p">[:,</span><span class="n">ineur</span><span class="p">],</span>
                  <span class="n">resp_test</span><span class="p">[:,</span><span class="n">ineur</span><span class="p">])</span>
  <span class="k">if</span> <span class="n">k</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">'test'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'m'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">'prediction'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_3387</span><span class="o">/</span><span class="mf">3835599226.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># @markdown Execute this cell to examine predictions for random subsets of neurons</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> 
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grating_stimuli</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Visualize tuning curves &amp; plot neural predictions</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/tmp/ipykernel_3387/2166509181.py</span> in <span class="ni">forward</span><span class="nt">(self, r)</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span> 
<span class="g g-Whitespace">     </span><span class="mi">32</span>     <span class="s2">"""</span>
<span class="ne">---&gt; </span><span class="mi">33</span><span class="s2">     h = torch.relu(self.in_layer(r))</span>
<span class="g g-Whitespace">     </span><span class="mi">34</span><span class="s2">     logp = self.logprob(self.out_layer(h))</span>
<span class="g g-Whitespace">     </span><span class="mi">35</span><span class="s2">     return logp</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span><span class="s2">         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span><span class="s2">                 or _global_forward_hooks or _global_forward_pre_hooks):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span><span class="s2">             return forward_call(*input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span><span class="s2">         # Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span><span class="s2">         full_backward_hooks, non_full_backward_hooks = [], []</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/torch/nn/modules/linear.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">95</span><span class="s2">     def forward(self, input: Tensor) -&gt; Tensor:</span>
<span class="ne">---&gt; </span><span class="mi">96</span><span class="s2">         return F.linear(input, self.weight, self.bias)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">98</span><span class="s2">     def extra_repr(self) -&gt; str:</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/torch/nn/functional.py</span> in <span class="ni">linear</span><span class="nt">(input, weight, bias)</span>
<span class="g g-Whitespace">   </span><span class="mi">1845</span><span class="s2">     if has_torch_function_variadic(input, weight):</span>
<span class="g g-Whitespace">   </span><span class="mi">1846</span><span class="s2">         return handle_torch_function(linear, (input, weight), input, weight, bias=bias)</span>
<span class="ne">-&gt; </span><span class="mi">1847</span><span class="s2">     return torch._C._nn.linear(input, weight, bias)</span>
<span class="g g-Whitespace">   </span><span class="mi">1848</span><span class="s2"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1849</span><span class="s2"> </span>

<span class="ne">RuntimeError</span>: mat1 and mat2 shapes cannot be multiplied (720x16 and 23589x20)
</pre></div>
</div>
</div>
</div>
<p>We can see if the convolutional channels changed at all from their initialization as center-surround and Gabor filters. If they don’t then it means that they were a sufficient basis set to explain the responses of the neurons to orientations to the accuracy seen above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get weights of conv layer in convLayer</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># how many convolutional channels to have in our layer</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># can you identify what each of the dimensions are?</span>

<span class="n">plot_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_3387</span><span class="o">/</span><span class="mf">356809345.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># get weights of conv layer in convLayer</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">out_channels</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># how many convolutional channels to have in our layer</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># can you identify what each of the dimensions are?</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> 

<span class="nn">/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">__getattr__</span><span class="nt">(self, name)</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>                 <span class="k">return</span> <span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1130</span>         <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">"'</span><span class="si">{}</span><span class="s2">' object has no attribute '</span><span class="si">{}</span><span class="s2">'"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
<span class="ne">-&gt; </span><span class="mi">1131</span>             <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> 
<span class="g g-Whitespace">   </span><span class="mi">1133</span>     <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s1">'Module'</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

<span class="ne">AttributeError</span>: 'DeepNetSoftmax' object has no attribute 'conv'
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"98457c8ea1204f478566c83d963f335a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d185a56443b04e128b31fcdfe7b82160": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_98457c8ea1204f478566c83d963f335a", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Eh41167WP\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f4b547cac50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Eh41167WP&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "}}]}}, "b88600786c5a47429c25a83e1be82444": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0be5538bece84e928176fe50bd789742": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_b88600786c5a47429c25a83e1be82444", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=UNBOPZf0QNQ\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f4b5472e190>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/UNBOPZf0QNQ?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "f45d25f47eff46dbab499ba49aff8334": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5d93fbc04ceb4455a1d28d6b4a12eff0": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_0be5538bece84e928176fe50bd789742", "IPY_MODEL_d185a56443b04e128b31fcdfe7b82160"], "layout": "IPY_MODEL_f45d25f47eff46dbab499ba49aff8334", "selected_index": 0}}}, "version_major": 2, "version_minor": 0}
</script>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W2D1_DeepLearning/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="W2D1_Tutorial3.html" id="prev-link" title="previous page">Tutorial 3: Building and Evaluating Normative Encoding Models</a>
<a class="right-next" href="../W2D1_Outro.html" id="next-link" title="next page">Outro</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>